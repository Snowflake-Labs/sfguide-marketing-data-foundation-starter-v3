{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Marketing Data Foundation Setup Script\n",
    "\n",
    "## Project Setup using Notebook\n",
    "\n",
    "### *Pre-requisites:*\n",
    "\n",
    "1. Role with access to create Compute Pools, create Warehouses and Databases\n",
    "2. Any environment to run notebooks like VS code, Jupyterlab, etc.\n",
    "3. Have Docker Desktop installed\n",
    "4. Install and configure the Snow CLI to deploy the application in your account.\n",
    "\n",
    "\n",
    "### Config files\n",
    "The project contains some config file that should be updated with your own environment variables\n",
    "1. [app.config.json](app.config.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "! python3.9 -m venv .venv \n",
    "! source .venv/bin/activate \n",
    "! pip install --upgrade pip && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load App Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update Account Locator\n",
    "import os\n",
    "import json\n",
    "from scripts.update_file_variables import file_replace\n",
    "from scripts.executeStatement import executeStatement\n",
    "\n",
    "app_settings = 'app.config.json'\n",
    "result = executeStatement(f\"SELECT CONCAT(CURRENT_ORGANIZATION_NAME(),'-',CURRENT_ACCOUNT_NAME()) as ACCOUNT\", \"--format json\")\n",
    "account_name_json = json.loads(result)\n",
    "account_name = account_name_json[0].get('ACCOUNT')\n",
    "replace_map = {\n",
    "    \"<account_registry>\": account_name.replace(\"_\", \"-\").replace(\".\", \"-\"),\n",
    "}\n",
    "file_replace(app_settings, replace_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.app_config import get_app_config\n",
    "\n",
    "app_config_file = 'app.config.json'\n",
    "app_config = get_app_config(app_config_file)\n",
    "\n",
    "print(f'Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.auth import get_conn_obj\n",
    "from scripts.executeStatement import executeStatement\n",
    "import os\n",
    "\n",
    "user = os.environ['USER']\n",
    "role = f\"{app_config['database']}_ROLE\"\n",
    "print(executeStatement(f\"CREATE ROLE IF NOT EXISTS {role};\"))\n",
    "print(executeStatement(f\"GRANT CREATE COMPUTE POOL ON ACCOUNT TO ROLE {role};\"))\n",
    "print(executeStatement(f\"GRANT BIND SERVICE ENDPOINT ON ACCOUNT TO ROLE {role};\"))\n",
    "print(executeStatement(f\"GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE {role};\"))\n",
    "print(executeStatement(f\"GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE {role};\"))\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace application configuration files keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from scripts.update_file_variables import file_replace\n",
    "\n",
    "app_config_f = 'app/src/manifest.yml'\n",
    "snowflake_f = 'app/snowflake.yml'\n",
    "fullstack_config_f = 'app/src/fullstack.yaml'\n",
    "makefile_f = 'Makefile'\n",
    "\n",
    "replace_map = {\n",
    "    \"<image_repository_path>\": os.environ[\"IMAGE_REPOSITORY\"],\n",
    "    \"<image_repo_short_path>\": os.environ[\"IMAGE_REPO_SHORT\"],\n",
    "    \"<role>\": role\n",
    "}\n",
    "file_replace(snowflake_f, replace_map)\n",
    "file_replace(app_config_f, replace_map)\n",
    "file_replace(fullstack_config_f, replace_map)\n",
    "file_replace(makefile_f, replace_map)\n",
    "print(f'Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear resources (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! cd app && snow app teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.executeStatement import executeStatement\n",
    "compute_pool_name = app_config['compute_pool']\n",
    "print(executeStatement(f\"DROP DATABASE IF EXISTS FIVETRAN_CONNECTOR_DEMO\"))\n",
    "print(executeStatement(f\"DROP DATABASE IF EXISTS OMNATA_CONNECTOR_DEMO\"))\n",
    "print(executeStatement(f\"DROP DATABASE IF EXISTS DATA_QUALITY_NOTEBOOKS\"))\n",
    "print(executeStatement(f\"DROP DATABASE IF EXISTS LLM_DEMO\"))\n",
    "print(executeStatement(f\"DROP WAREHOUSE IF EXISTS {app_config['dynamic_table_warehouse']}\"))\n",
    "print(executeStatement(f\"ALTER COMPUTE POOL IF EXISTS {compute_pool_name} STOP ALL;\"))\n",
    "print(executeStatement(f\"DROP SERVICE IF EXISTS {app_config['container_service']}\"))\n",
    "print(executeStatement(f\"DROP COMPUTE POOL IF EXISTS {compute_pool_name}\"))\n",
    "print(executeStatement(f\"DROP IMAGE REPOSITORY IF EXISTS {app_config['image_stage']};\"))\n",
    "print(executeStatement(f\"DROP DATABASE IF EXISTS {app_config['database']};\"))\n",
    "print(executeStatement(f\"DROP WAREHOUSE IF EXISTS {app_config['warehouse']};\"))\n",
    "print(executeStatement(f\"DROP DATABASE IF EXISTS {app_config['sample_db']};\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create database, schema and stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_app_package = f\"CREATE DATABASE IF NOT EXISTS {app_config['database']};\"\n",
    "create_dedicated_warehouse = f\"CREATE WAREHOUSE IF NOT EXISTS {app_config['warehouse']} WITH WAREHOUSE_SIZE = 'XSMALL' AUTO_SUSPEND = 60 AUTO_RESUME = TRUE INITIALLY_SUSPENDED = TRUE;\"\n",
    "grant_database = f\"GRANT ALL PRIVILEGES ON DATABASE {app_config['database']} TO ROLE {role}\"\n",
    "\n",
    "create_app_schema = f\"CREATE SCHEMA IF NOT EXISTS {app_config['database']}.{app_config['schema']};\"\n",
    "\n",
    "grant_schema = f\"GRANT ALL PRIVILEGES ON SCHEMA {app_config['database']}.{app_config['schema']} TO ROLE {role};\"\n",
    "\n",
    "grant_tables = f\"GRANT SELECT ON ALL TABLES IN SCHEMA {app_config['database']}.{app_config['schema']} TO ROLE {role};\"\n",
    "\n",
    "create_image_repo = f\"CREATE IMAGE REPOSITORY IF NOT EXISTS {app_config['database']}.{app_config['schema']}.{app_config['image_stage']};\"\n",
    "\n",
    "print(executeStatement(create_app_package))\n",
    "print(executeStatement(create_app_schema))\n",
    "print(executeStatement(create_image_repo))\n",
    "print(executeStatement(grant_database))\n",
    "print(executeStatement(grant_schema))\n",
    "print(executeStatement(grant_tables))\n",
    "print(executeStatement(create_dedicated_warehouse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Upload Docker Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! make all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Native Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! cd app && snow app run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "appName = f\"MARKETING_DATA_FOUNDATION_STARTER_V3_{os.environ['USER'].upper()}\"\n",
    "print(appName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Predefined Data Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload sample notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_app_package = f\"CREATE DATABASE IF NOT EXISTS {app_config['data_quality_database']};\"\n",
    "\n",
    "create_app_schema = f\"CREATE SCHEMA IF NOT EXISTS {app_config['data_quality_database']}.{app_config['schema']};\"\n",
    "\n",
    "create_app_stage = f\"CREATE STAGE IF NOT EXISTS {app_config['data_quality_database']}.{app_config['schema']}.{app_config['code_stage']} \\\n",
    "    DIRECTORY = (ENABLE = TRUE) \\\n",
    "    COMMENT = 'Used for holding data quality demo notebooks';\"\n",
    "\n",
    "create_wh = f\"CREATE OR REPLACE WAREHOUSE {app_config['dynamic_table_warehouse']} WITH WAREHOUSE_SIZE= MEDIUM;\"\n",
    "\n",
    "print(executeStatement(create_app_package))\n",
    "print(executeStatement(create_app_schema))\n",
    "print(executeStatement(create_app_stage))\n",
    "print(executeStatement(create_wh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.upload_files import upload_files_stage\n",
    "\n",
    "database = app_config['data_quality_database']\n",
    "schema = app_config['schema']\n",
    "stage = app_config['code_stage']\n",
    "native_app_dir = './notebooks'\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Directories to ignore\n",
    "dirs_ignore = ['/streamlit/frontend', 'pycache', 'tests']\n",
    "is_ignore = lambda path: len(list(filter(lambda ignore: ignore in path, dirs_ignore))) > 0\n",
    "\n",
    "def upload_files_stage(database: str, schema: str, stage: str, app_dir: str) -> None:\n",
    "    for path, currentDirectory, files in os.walk(app_dir):\n",
    "        for file in files:\n",
    "            dir = app_dir.replace(\"./\",\"\")\n",
    "            if not file.startswith('.') and not is_ignore(path):\n",
    "                # build the relative paths to the file\n",
    "                local_file = os.path.join(path, file)\n",
    "                replace_path = os.path.join('.',dir)\n",
    "\n",
    "                # build the path to where the file will be staged\n",
    "                stage_dir = path.replace(replace_path,'')\n",
    "                print(f'{local_file} => @{stage}{stage_dir}')\n",
    "                stage_location = f'@{database}.{schema}.{stage}/{stage_dir}'\n",
    "                print(local_file)\n",
    "                print(stage_location)\n",
    "                subprocess.run(['snow', 'stage', 'copy', local_file, stage_location])\n",
    "                executeStatement(f'alter stage {database}.{schema}.{stage} refresh; ')\n",
    "\n",
    "upload_files_stage(database,schema,stage, native_app_dir)\n",
    "\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD SAMPLE DATA\n",
    "create_sample_db_facebook = \"CREATE DATABASE IF NOT EXISTS FIVETRAN_CONNECTOR_DEMO\"\n",
    "create_sample_schema_facebook = \"CREATE SCHEMA IF NOT EXISTS FIVETRAN_CONNECTOR_DEMO.FACEBOOK_RAW\"\n",
    "create_sample_schema_linkedin =\"CREATE SCHEMA IF NOT EXISTS OMNATA_CONNECTOR_DEMO.LINKEDIN_RAW\"\n",
    "create_samle_db_linkedin = \"CREATE DATABASE IF NOT EXISTS OMNATA_CONNECTOR_DEMO\"\n",
    "create_llm_demo_db = \"CREATE DATABASE IF NOT EXISTS LLM_DEMO\"\n",
    "create_llm_schema = \"CREATE SCHEMA IF NOT EXISTS LLM_DEMO.DEMO\"\n",
    "grant_db_facebook = f\"GRANT USAGE ON DATABASE FIVETRAN_CONNECTOR_DEMO TO APPLICATION {appName}\"\n",
    "grant_db_linkedin = f\"GRANT USAGE ON DATABASE OMNATA_CONNECTOR_DEMO TO APPLICATION {appName}\"\n",
    "grant_schema_facebook = f\"GRANT USAGE ON SCHEMA FIVETRAN_CONNECTOR_DEMO.FACEBOOK_RAW TO APPLICATION {appName}\"\n",
    "grant_schema_linkedin = f\"GRANT USAGE ON SCHEMA OMNATA_CONNECTOR_DEMO.LINKEDIN_RAW TO APPLICATION {appName}\"\n",
    "print(executeStatement(create_sample_db_facebook))\n",
    "print(executeStatement(create_samle_db_linkedin))\n",
    "print(executeStatement(create_sample_schema_facebook))\n",
    "print(executeStatement(create_sample_schema_linkedin))\n",
    "print(executeStatement(grant_db_facebook))\n",
    "print(executeStatement(grant_db_linkedin))\n",
    "print(executeStatement(grant_schema_facebook))\n",
    "print(executeStatement(grant_schema_linkedin))\n",
    "print(executeStatement(create_llm_demo_db))\n",
    "print(executeStatement(create_llm_schema))\n",
    "\n",
    "executeStatement(f\"\"\"CREATE OR REPLACE NOTEBOOK  LLM_DEMO.DEMO.DATA_QUALITY_DEMO_1\n",
    "    FROM '@data_quality_notebooks.METADATA.CODE_STG'\n",
    "    MAIN_FILE = 'data_quality_demo_1.ipynb'\n",
    "    QUERY_WAREHOUSE = '{app_config['warehouse']}';\"\"\")\n",
    "\n",
    "executeStatement(\"ALTER NOTEBOOK LLM_DEMO.DEMO.DATA_QUALITY_DEMO_1 ADD LIVE VERSION FROM LAST\")\n",
    "\n",
    "executeStatement(f\"\"\"CREATE OR REPLACE NOTEBOOK LLM_DEMO.DEMO.DATA_QUALITY_DEMO_2\n",
    "    FROM '@data_quality_notebooks.METADATA.CODE_STG'\n",
    "    MAIN_FILE = 'data_quality_demo_2.ipynb'\n",
    "    QUERY_WAREHOUSE = '{app_config['warehouse']}';\"\"\")\n",
    "\n",
    "executeStatement(\"ALTER NOTEBOOK LLM_DEMO.DEMO.DATA_QUALITY_DEMO_2 ADD LIVE VERSION FROM LAST\")\n",
    "\n",
    "executeStatement(f\"\"\"CREATE OR REPLACE NOTEBOOK LLM_DEMO.DEMO.DATA_QUALITY_DEMO_3\n",
    "    FROM '@data_quality_notebooks.METADATA.CODE_STG'\n",
    "    MAIN_FILE = 'data_quality_demo_3.ipynb'\n",
    "    QUERY_WAREHOUSE = '{app_config['warehouse']}';\"\"\")\n",
    "\n",
    "executeStatement(\"ALTER NOTEBOOK LLM_DEMO.DEMO.DATA_QUALITY_DEMO_3 ADD LIVE VERSION FROM LAST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Demo Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = \"./data\"\n",
    "print(\"Loading Tables\")\n",
    "\n",
    "def executeMultilineStatement(statement):\n",
    "    return subprocess.run(['snow', 'sql', '-q', statement])\n",
    "\n",
    "\n",
    "database = app_config['data_quality_database']\n",
    "schema = app_config['schema']\n",
    "stage = app_config['code_stage']\n",
    "stage_location = f'@{database}.{schema}.{stage}/sample_data'\n",
    "\n",
    "csv_file_format = f\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT {database}.{schema}.csv_format\n",
    "  TYPE = csv\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  PARSE_HEADER = true;\n",
    "\"\"\"\n",
    "statement = ''.join(csv_file_format.splitlines())\n",
    "executeMultilineStatement(statement)\n",
    "for path, currentDirectory, files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        if file in [\".DS_Store\"]:\n",
    "            continue\n",
    "        else:\n",
    "            database = appName.upper()\n",
    "            file_path = os.path.join(path, file)\n",
    "            print(\"Loading File:\" + file_path)            \n",
    "            table_name = file.split(\".\")[0].upper()\n",
    "            if table_name == \"DIM_PLATFORM\":\n",
    "                schema = \"CAMPAIGN_INTELLIGENCE_COMBINED\"\n",
    "            else:\n",
    "                schema = file_path.replace(data_dir+\"/\",\"\").replace(\"/\"+file,\"\").upper()\n",
    "                schema = schema.lstrip(\"/\")\n",
    "            if len(schema.split(\"/\")) > 1:\n",
    "                database, schema = [i.upper() for i in schema.split(\"/\")]\n",
    "            fileName = os.path.basename(file_path)\n",
    "            subprocess.run(['snow', 'stage', 'copy', file_path, stage_location])\n",
    "            createTable = f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {database}.{schema}.{table_name}\n",
    "                USING TEMPLATE (\n",
    "                    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n",
    "                    FROM TABLE(\n",
    "                        INFER_SCHEMA(\n",
    "                        LOCATION=>'{stage_location}/{fileName}',\n",
    "                        FILE_FORMAT => 'DATA_QUALITY_NOTEBOOKS.METADATA.csv_format'\n",
    "                        )\n",
    "                    ));\n",
    "            \"\"\"\n",
    "            executeMultilineStatement(createTable)\n",
    "            copyTo = f\"\"\"COPY INTO {database}.{schema}.{table_name} FROM {stage_location}/{fileName} FILE_FORMAT = (FORMAT_NAME= 'DATA_QUALITY_NOTEBOOKS.METADATA.csv_format')  MATCH_BY_COLUMN_NAME=\"CASE_INSENSITIVE\"; \"\"\"\n",
    "            executeStatement(copyTo)\n",
    "            if database == \"OMNATA_CONNECTOR_DEMO\":\n",
    "                executeStatement(f\"ALTER TABLE {database}.{schema}.{table_name} ADD COLUMN RECORD_DATA_VARIANT VARIANT;\")\n",
    "                executeStatement(f\"UPDATE {database}.{schema}.{table_name} SET RECORD_DATA_VARIANT = TO_VARIANT(PARSE_JSON(RECORD_DATA));\")\n",
    "                executeStatement(f\"ALTER TABLE {database}.{schema}.{table_name} DROP COLUMN RECORD_DATA;\")\n",
    "                executeStatement(f\"ALTER TABLE {database}.{schema}.{table_name} RENAME COLUMN RECORD_DATA_VARIANT to RECORD_DATA;\")\n",
    "            changeTracking = f\"ALTER TABLE {database}.{schema}.{table_name} SET CHANGE_TRACKING = TRUE;\"\n",
    "            executeStatement(changeTracking)\n",
    "  \n",
    "\n",
    "grant_select_db_linkedin = f\"GRANT SELECT ON ALL TABLES IN SCHEMA OMNATA_CONNECTOR_DEMO.LINKEDIN_RAW TO APPLICATION {appName}\"\n",
    "grant_select_db_facebook = f\"GRANT SELECT ON ALL TABLES IN SCHEMA FIVETRAN_CONNECTOR_DEMO.FACEBOOK_RAW TO APPLICATION {appName}\"\n",
    "print(executeStatement(grant_select_db_facebook))\n",
    "print(executeStatement(grant_select_db_linkedin))\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.load_models import load_models\n",
    "from scripts.update_file_variables import file_replace\n",
    "\n",
    "models_folder = \"backend/predefined_models\"\n",
    "\n",
    "stage = f\"{appName}.{app_config['schema']}.TEMP\"\n",
    "create_sample_stage = f\"CREATE STAGE IF NOT EXISTS {stage};\"\n",
    "print(executeStatement(create_sample_stage))\n",
    "\n",
    "json_file_format = f\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT {appName}.{app_config['schema']}.json_format\n",
    "  TYPE = JSON;\n",
    "\"\"\"\n",
    "print(executeStatement(json_file_format))\n",
    "\n",
    "replace_map = {\n",
    "  \"<DB>\": appName,\n",
    "  \"<SCHEMA>\": app_config['schema']\n",
    "}\n",
    "\n",
    "file_replace(\"scripts/models_raw.sql\", replace_map)\n",
    "\n",
    "load_models(appName, models_folder, f\"@{stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! snow sql -f scripts/models_raw.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.executeStatement import executeStatement\n",
    "create_compute_pool_sql = f\"CREATE COMPUTE POOL IF NOT EXISTS {compute_pool_name} for application {appName}\\\n",
    "    MIN_NODES = 1 \\\n",
    "    MAX_NODES = 1 \\\n",
    "    AUTO_SUSPEND_SECS = 120 \\\n",
    "    INSTANCE_FAMILY = CPU_X64_S;\"\n",
    "grant_usage_sql = f\"GRANT USAGE, MONITOR ON COMPUTE POOL {compute_pool_name} \\\n",
    "    TO application {appName};\"\n",
    "\n",
    "grant_usage_wh = f\"GRANT USAGE, MONITOR ON WAREHOUSE {app_config['warehouse']} \\\n",
    "    TO application {appName};\"\n",
    "\n",
    "grant_bind_service = f\"GRANT BIND SERVICE ENDPOINT ON ACCOUNT TO application {appName};\"\n",
    "\n",
    "grant_cortex = f\"GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE {role};\"\n",
    "\n",
    "print(executeStatement(create_compute_pool_sql))\n",
    "print(executeStatement(grant_usage_sql))\n",
    "print(executeStatement(grant_usage_wh))\n",
    "print(executeStatement(grant_bind_service))\n",
    "print(executeStatement(grant_cortex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_procedure_commands = f\"\"\"\n",
    "CREATE OR REPLACE PROCEDURE {appName}.METADATA.CREATE_DYNAMIC_TABLE(query VARCHAR)\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = 3.8\n",
    "HANDLER = 'creator'\n",
    "PACKAGES = ('snowflake-snowpark-python')\n",
    "AS\n",
    "\\$\\$\n",
    "def creator(session, query):\n",
    "    return session.sql(query).collect()\n",
    "\\$\\$;\n",
    "\"\"\"\n",
    "grant_procedure_usage = f\"GRANT USAGE ON PROCEDURE {appName}.METADATA.CREATE_DYNAMIC_TABLE(VARCHAR) TO APPLICATION {appName};\"\n",
    "print(executeStatement(create_procedure_commands))\n",
    "print(executeStatement(grant_procedure_usage))\n",
    "\n",
    "\n",
    "# Create grants procedure\n",
    "create_procedure_commands = f\"\"\"\n",
    "CREATE OR REPLACE PROCEDURE {appName}.METADATA.GRANTER(appName VARCHAR, tables VARIANT)\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = 3.8\n",
    "HANDLER = 'granter'\n",
    "PACKAGES = ('snowflake-snowpark-python')\n",
    "AS\n",
    "\\$\\$\n",
    "\n",
    "def granter(session, appName, tables):\n",
    "    for table_name in tables:\n",
    "        session.sql(f'GRANT SELECT ON TABLE {{table_name}} TO APPLICATION {{appName}}').collect()\n",
    "\\$\\$;\n",
    "\"\"\"\n",
    "grant_procedure_usage = f\"GRANT USAGE ON PROCEDURE {appName}.METADATA.GRANTER(VARCHAR, VARIANT) TO APPLICATION {appName};\"\n",
    "print(executeStatement(create_procedure_commands))\n",
    "print(executeStatement(grant_procedure_usage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload Semantic Model Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.upload_files import upload_files_stage\n",
    "from scripts.update_file_variables import file_replace\n",
    "\n",
    "database = appName\n",
    "schema = 'LLM'\n",
    "semantic_models_stage = 'SEMANTIC_MODEL'\n",
    "llm_config_stage = 'CONFIGURATION'\n",
    "\n",
    "replace_map = {\n",
    "    \"<target_database>\": appName\n",
    "}\n",
    "file_replace('./assistant/semantic_models/UnifiedMarketingModel_CAMPAIGN_PERF.yaml', replace_map)\n",
    "\n",
    "subprocess.run(['snow', 'stage', 'copy', './assistant/config/assistant_config.yaml', f\"@{database}.{schema}.{llm_config_stage}\"])\n",
    "subprocess.run(['snow', 'stage', 'copy', './assistant/semantic_models/UnifiedMarketingModel_CAMPAIGN_PERF.yaml', f\"@{database}.{schema}.{semantic_models_stage}\"])\n",
    "\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer 360 Demo Upload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.update_file_variables import file_replace\n",
    "from scripts.executeStatement import executeCopyToStage, executeStatement\n",
    "\n",
    "create_sample_db = f\"CREATE DATABASE IF NOT EXISTS {app_config['sample_db']};\"\n",
    "create_sample_schema = f\"CREATE SCHEMA IF NOT EXISTS {app_config['sample_db']}.{app_config['sample_schema']};\"\n",
    "create_sample_stage = f\"CREATE STAGE IF NOT EXISTS {app_config['sample_db']}.{app_config['sample_schema']}.{app_config['sample_stage']};\"\n",
    "\n",
    "print(executeStatement(create_sample_db))\n",
    "print(executeStatement(create_sample_schema))\n",
    "print(executeStatement(create_sample_stage))\n",
    "\n",
    "\n",
    "sample_stage= f\"@{app_config['sample_db']}.{app_config['sample_schema']}.{app_config['sample_stage']}\"\n",
    "ga_data = \"c360demo/data/ga_data/\"\n",
    "sf_data = \"c360demo/data/sf_data/\"\n",
    "worldcities = \"c360demo/data/worldcities.csv\"\n",
    "\n",
    "print(executeCopyToStage(ga_data,f\"{sample_stage}/data/ga_data/\"))\n",
    "print(executeCopyToStage(sf_data, f\"{sample_stage}/data/sf_data/\"))\n",
    "print(executeCopyToStage(worldcities, f\"{sample_stage}/data\"))\n",
    "\n",
    "data_script_f = 'scripts/build_raw_samples.sql'\n",
    "\n",
    "replace_map = {\n",
    "    \"<DB>\": app_config['sample_db'],\n",
    "    \"<SCHEMA>\": app_config['sample_schema'],\n",
    "    \"<STAGE>\": app_config['sample_stage']\n",
    "}\n",
    "\n",
    "file_replace(data_script_f, replace_map)\n",
    "print(f'Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grant_db_c360 = f\"GRANT USAGE ON DATABASE {app_config['sample_db']} TO APPLICATION {appName}\"\n",
    "print(executeStatement(grant_db_c360))\n",
    "grant_schema_360 = f\"GRANT USAGE ON SCHEMA {app_config['sample_db']}.{app_config['sample_schema']} TO APPLICATION {appName}\"\n",
    "grant_select_360 = f\"GRANT SELECT ON ALL TABLES IN SCHEMA {app_config['sample_db']}.{app_config['sample_schema']} TO APPLICATION {appName}\"\n",
    "print(executeStatement(grant_schema_360))\n",
    "print(executeStatement(grant_select_360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! snow sql -f scripts/build_raw_samples.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start container service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_query = f\"call {appName}.app_public.start_app(\\'{compute_pool_name}\\',\\'{app_config['warehouse']}\\')\"\n",
    "executeStatement(service_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show container endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.executeStatement import executeStatement\n",
    "from scripts.endpoint_provider import get_public_url_na\n",
    "get_public_url_na(appName, executeStatement)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
